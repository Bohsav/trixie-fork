{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Trixie HPC wiki","text":""},{"location":"#trixie-status-outages-oct-17-21-and-oct-22","title":"Trixie Status Outages Oct 17-21, and Oct 22","text":""},{"location":"#trixie-not-just-an-gpu-cluster","title":"Trixie - not just an GPU Cluster","text":"<p>The name for this AI GPU cluster comes from a very inspirational women Beatrice \"Trixie\" Helen Worsley who helped shape the state of computer science in Canada and abroad. She even worked briefly for the National Research Council of Canada in the late forties.</p>"},{"location":"#trixie-availability","title":"Trixie availability","text":"<p>Trixie is designated for collaborative research in support of the NRC AI4D Challenge Program.  It is also available for the NRC Pandemic Response (COVID) Challenge Program and some other internal NRC research initiatives on a prioritized basis.  Access is managed via projects where project participants who need access to Trixie (internal and external to NRC) are specified as part of the Trixie access request.  NRC participants must be employees of NRC or NRC volunteer visitors with current agreements at NRC.  External users must be a principle investigator (PI) named in a grant, contribution and/or collaborative agreement with NRC, or be a student, post doc, or research associate directly supervised by that PI.</p> <p>Trixie access requests can be completed by the NRC PI in consultation with their external collaborators.  Access will only be granted for approved NRC (internal or collaborative) projects.</p> <p>The access request form is available here. Once completed, the form can be submitted to the AI4D program manager, Patricia Oakley.</p>"},{"location":"#nrc-authorized-users-only-utilisateurs-autorises-du-cnrc-seulement","title":"NRC authorized users only - Utilisateurs autoris\u00e9s du CNRC seulement","text":"<p>Access to the National Research Council of Canada Information Technology (IT) systems and resources by employees and any other person must be authorized. All users shall comply with the NRC Policy on Acceptable Network and Device Use (PANDU). All activities done using these systems and resources are subject to monitoring.</p> <p>NOTICE: Anyone using these systems and resources by their access consents to such monitoring, and has read and understands the responsibilities outlined within the PANDU. Unauthorized use and PANDU violations may result in disciplinary action and/or criminal prosecution.</p> <p>Les employ\u00e9s du Conseil national de recherches du Canada (CNRC) et autres utilisateurs qui acc\u00e8dent aux syst\u00e8mes et aux ressources infotechnologiques du CNRC doivent \u00eatre autoris\u00e9s \u00e0 le faire. Ils doivent en outre respecter la Politique sur l\u2019utilisation acceptable des dispositifs et des r\u00e9seaux (PUADR) du CNRC. Toute utilisation des syst\u00e8mes et ressources peut faire l\u2019objet d\u2019une surveillance. AVIS : Quiconque utilise les syst\u00e8mes et ressources infotechnologiques du CNRC consent par le fait m\u00eame \u00e0 faire l\u2019objet d\u2019une surveillance et atteste avoir pris connaissance de ses responsabilit\u00e9s en vertu de la PUADR. Toute utilisation non autoris\u00e9e du mat\u00e9riel ou tout manquement \u00e0 la PUADR pourrait entra\u00eener des mesures disciplinaires, voire une poursuite au criminel.</p>"},{"location":"#policy-on-acceptable-network-and-device-use-pandu","title":"Policy on acceptable network and device use (PANDU)","text":"<p>All users shall comply with the NRC Policy on Acceptable Network and Device Use (PANDU)</p> <p>PANDU</p>"},{"location":"#politique-sur-lutilisation-acceptable-des-dispositifs-et-des-reseaux-puadr","title":"Politique sur l'utilisation acceptable des dispositifs et des r\u00e9seaux (PUADR)","text":"<p>Tous les utilisateurs doivent respecter la Politique sur l\u2019utilisation acceptable des dispositifs et des r\u00e9seaux (PUADR)</p> <p>PUADR</p>"},{"location":"#getting-an-account","title":"Getting an account","text":"<p>Accounts will be generated as required based on approved Trixie access request forms.  If you have an approved access request, but have not received account information, you can contact the AI4D Program Manager, Patricia Oakley for assistance.</p>"},{"location":"#username-and-password","title":"Username and Password","text":"<p>Your username and password are the same as your RES (orange) account.</p>"},{"location":"#connecting-to-the-machine","title":"Connecting to the machine","text":"<p>The connection path will vary depending from which network you are trying to access.</p> <ul> <li>From Legacy (black) and RES (orange) (wired or over 2 Factor VPN) users can connect directly via SSH using the host name: trixie.res.nrc.gc.ca</li> <li>Non-NRC collaborators can access Trixie via a bastion host. Please speak to your NRC contact to get access and additional details. Once you receive your access credentials please see External Access Setup to configure your local computer to access Trixie.</li> </ul>"},{"location":"#file-transfers","title":"File Transfers","text":""},{"location":"#running-jobs","title":"Running jobs","text":"<p>Jobs on Trixie must be run via the SLURM job scheduler. Do NOT run jobs on the headnode. Users who run on the head node risk account suspension. Running jobs</p>"},{"location":"#description-of-the-cluster","title":"Description of the cluster","text":"<p>Hardware</p>"},{"location":"#available-software","title":"Available software","text":"<p>Available software</p>"},{"location":"#pytorchdistributed","title":"pytorch.distributed","text":"<p>SLURM, pytorch distributed and Multiple Nodes</p>"},{"location":"Account-Codes/","title":"Account Codes","text":"<p>In order to run jobs on Trixie, users need to specify which SLURM Account Code should be used for billing. This is handled by adding a line in the SLURM submission script which identifies the account</p> <pre><code>SBATCH --account=account_code\n</code></pre> <p>Users must be authorized to charge an account before they can use it. </p>"},{"location":"Account-Codes/#ai4design","title":"AI4Design","text":""},{"location":"Account-Codes/#ai4d-bio-01","title":"ai4d-bio-01","text":"<p>AI for Drug Design, NRC-PI:Tchagang, Alain</p>"},{"location":"Account-Codes/#ai4d-bio-02","title":"ai4d-bio-02","text":"<p>Precision Discovery in Bio Systems, NRC-PI:Shao, Xiaojian</p>"},{"location":"Account-Codes/#ai4d-bio-03","title":"ai4d-bio-03","text":"<p>Multi-Targeted Therapeutics, NRC-PI:Fauteux, Fran\u00e7ois</p>"},{"location":"Account-Codes/#ai4d-bio-04a","title":"ai4d-bio-04a","text":"<p>Protein Design Drugs &amp; Gene, NRC-PI:Paquet, Eric</p>"},{"location":"Account-Codes/#ai4d-bio-04b","title":"ai4d-bio-04b","text":"<p>AI Simulation of Bio Systems, NRC-PI:Cuperlovic-Culf, Miroslav</p>"},{"location":"Account-Codes/#ai4d-bio-04c","title":"ai4d-bio-04c","text":"<p>Digital-Twining of Bioreactor, NRC-PI:Belacel, Nabil</p>"},{"location":"Account-Codes/#ai4d-core-01","title":"ai4d-core-01","text":"<p>AI-based Shape Optimization, NRC-PI:Shu, Chang</p>"},{"location":"Account-Codes/#ai4d-core-05","title":"ai4d-core-05","text":"<p>Design of Superconductive Tapes, NRC-PI:Valdes, Julio</p>"},{"location":"Account-Codes/#ai4d-core-06","title":"ai4d-core-06","text":"<p>Intelligent Design, NRC-PI:Guo, Hong Yu</p>"},{"location":"Account-Codes/#ai4d-mat-02","title":"ai4d-mat-02","text":"<p>Automated Material Synthesis using Deep Reinforcement Learning, NRC-PI:Tamblyn, Isaac</p>"},{"location":"Account-Codes/#ai4d-mat-03","title":"ai4d-mat-03","text":"<p>Simulation &amp; Design of Materials, NRC-PI:Tchagang, Alain</p>"},{"location":"Account-Codes/#ai4d-mat-04","title":"ai4d-mat-04","text":"<p>Spectroscopic Signatures, NRC-PI:Tamblyn, Isaac</p>"},{"location":"Account-Codes/#ai4d-photo-01a","title":"ai4d-photo-01a","text":"<p>Miniaturization HP Components, NRC-PI:Grinberg, Yuri</p>"},{"location":"Account-Codes/#ai4d-photo-01c","title":"ai4d-photo-01c","text":"<p>AI-assisted Inverse Design, NRC-PI:Grinberg, Yuri</p>"},{"location":"Account-Codes/#covid","title":"COVID","text":""},{"location":"Account-Codes/#covid-01","title":"covid-01","text":"<p>NRC-PI:Ebadi, Ashkan; Xi, Pengchengi</p>"},{"location":"Account-Codes/#dt-digital-technologies-technologies-numeriques","title":"DT Digital Technologies / Technologies Num\u00e9riques","text":""},{"location":"Account-Codes/#dt-dac","title":"dt-dac","text":"<p>Data Analytics Centre / Donn\u00e9es Analytiques</p>"},{"location":"Account-Codes/#dt-dscs","title":"dt-dscs","text":"<p>Data Science for Complex Systems / Science des Donn\u00e9es pour les Syst\u00e8mes Complexes </p>"},{"location":"Account-Codes/#dt-mtp","title":"dt-mtp","text":"<p>Multilingual Text Processing / Traitement Multilingue de Texte </p>"},{"location":"Account-Codes/#dt-ta","title":"dt-ta","text":"<p>Text Analytics / Analyse de textes</p>"},{"location":"Account-Codes/#sdt-security-and-disruptive-technologies-technologies-de-securite-et-de-rupture","title":"SDT Security and Disruptive Technologies / Technologies de s\u00e9curit\u00e9 et de rupture","text":""},{"location":"Account-Codes/#sdt-clean","title":"sdt-clean","text":"<p>Computational Laboratory for Energy And Nanoscience</p>"},{"location":"Automatically-Resuming-Requeueing/","title":"How to I get my job to requeue after my time limit?","text":"<p>Here\u2019s a skeleton of what our jobs look like.  Please check your job once it is running to dial down the number of cpus and memory needed.  If we don\u2019t use the node\u2019s full resources, it would be nice to be able to submit other cpu-only jobs, aka none gpu jobs on those nodes.</p> <p>Important steps in order to get automatic requeueing working: * Ask slurm to send you a signal 30 seconds before the end of your time limit <code>--signal=B:USR1@30</code> * Have a thread listen to the requested signal <code>trap _requeue USR1</code> * Send your MAIN process in the background and wait for it otherwise your <code>_requeue</code> function will NEVER get a chance to run.</p> <pre><code>#SBATCH --job-name=WMT21.training\n#SBATCH --comment=\"Thanks Samuel Larkin for showing me how to work with Slurm\"\n\n#SBATCH --partition=TrixieMain\n#SBATCH --account=dt-mtp\n#SBATCH --gres=gpu:4\n#SBATCH --time=12:00:00                                                                                                                                                      #SBATCH --exclude=cn125\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1                                                                                                                                                  #SBATCH --cpus-per-task=24\n#SBATCH --mem=40G\n# To reserve a whole node for yourself\n####SBATCH --exclusive\n#SBATCH --open-mode=append\n#SBATCH --requeue\n#SBATCH --signal=B:USR1@30\n#SBATCH --output=%x-%j.out                                                                                                                                                   \n\n# Requeueing on Trixie\n# [source](https://www.sherlock.stanford.edu/docs/user-guide/running-jobs/)\n# [source](https://hpc-uit.readthedocs.io/en/latest/jobs/examples.html#how-to-recover-files-before-a-job-times-out)\nfunction _requeue {\n   echo \"BASH - trapping signal 10 - requeueing $SLURM_JOBID\"\n   date\n   # This would allow to generically requeue any job but since we are using XLM\n   # which is slurm aware, XLM could save its model before requeueing.\n   scontrol requeue $SLURM_JOBID\n}\n\nif [[ -n \"$SLURM_JOBID\" ]]; then\n   trap _requeue USR1\nfi\n\n\ntime python \u2013m sockeye.train \u2026. &amp;\nwait\n</code></pre> <p>where <code>time python \u2013m sockeye.train \u2026.</code> is the process you want to run.</p>"},{"location":"Available-Software/","title":"Available Software","text":"<p>The most up-to-date way to see which software has been preinstalled on Trixie is by using the module command. When in double, it is the definitive list.</p> <p>Software on Trixie is organized using the <code>module</code> service. Users can load, unload, and swap libraries within their environment and job submission scripts via <code>modules</code>.</p> <p>If there is a piece of software you would like to use but it is not available from the list (and you can't figure out how to build it yourself in your home directory), you may create a request using the issues tab (https://github.com/ai4d-iasc/trixie/issues). Please do not create duplicate requests for software (but feel free to comment on a thread to ''upvote'' the priority list is clear.</p>"},{"location":"Available-Software/#compilers","title":"Compilers","text":"<ul> <li>gcc</li> <li>intel (not currently available, in procurement)</li> </ul>"},{"location":"Available-Software/#numerical-libraries","title":"Numerical libraries","text":"<ul> <li>intel-mkl (not currently available, in procurement)</li> </ul>"},{"location":"Available-Software/#deep-learning-frameworks","title":"Deep learning frameworks","text":"<ul> <li>tensorflow</li> <li>pytorch</li> </ul>"},{"location":"Available-Software/#python","title":"Python","text":"<ul> <li>system python</li> <li>CC stack python (now default):</li> <li>virtualenvs</li> <li>Anaconda, miniconda:</li> <li>System-wide Anaconda</li> <li>User miniconda</li> </ul>"},{"location":"Available-Software/#scientific-simulation-software","title":"Scientific simulation software","text":"<ul> <li>abinit</li> <li>lumerical</li> </ul>"},{"location":"External-Access-Advanced-Configuration/","title":"Overview","text":"<p>To ease usage of the bastion host to easily connect to Trixie, there are some steps which can be taken, especially making use of the SSH ProxyJump and ControlMaster parameters. Basically, you need to configure SSH to automatically connect with the Trixie server using the bastion host as a connector between your local computer and the Trixie server.</p> <p>Important Note: Before proceeding with this configuration, please ensure that you have performed the External Access Setup procedure.</p>"},{"location":"External-Access-Advanced-Configuration/#mac-osx-linux","title":"Mac OSX / Linux","text":"<p>To configure SSH to automatically connect to the Trixie server, please open your <code>.ssh/config</code> file with your preferred text editor and add the following lines on your local machine \u2013 not the servers \u2013 while substituting your given usernames in the User directive. You will also need to create the folder <code>.ssh/sockets</code> to complete the configuration.</p> <pre><code>Host trixie-bastion\n  HostName trixie.nrc-cnrc.gc.ca\n  User &lt;firstname&gt;.&lt;lastname&gt;@pub.nrc-cnrc.gc.ca \n  ControlMaster auto\n  ControlPath ~/.ssh/sockets/%r@%h-%p\n\nHost trixie\n  HostName trixie.res.nrc.gc.ca \n  User admin.&lt;firstname&gt;.&lt;lastname&gt;\n  ProxyJump trixie-bastion\n</code></pre> <p>Once your settings are configured, you will be able to login to the Trixie server with the following command</p> <p><code>ssh trixie</code></p> <p>Please note that you will be prompted as follows</p> <ol> <li>LoginTC prompt \u2013 enter 1</li> <li>Prompt for your PUB password</li> <li>Prompt for your RES admin password</li> </ol>"},{"location":"External-Access-Advanced-Configuration/#windows-putty","title":"Windows \u2013 Putty","text":"<p>To configure SSH to automatically connect to the Trixie server, please set the following settings in your Putty application, substituting your username where applicable.</p> <ol> <li> <p>Under Connection -&gt; SSH</p> <ol> <li>Set Remote command: <code>ssh \u2013A \u2013Y admin.&lt;firstname&gt;.&lt;lastname&gt;@trixie.res.nrc.gc.ca</code></li> <li>Select the option Share SSH connections if possible \u2013 this will enable you to establish multiple connections to Trixie</li> </ol> <p> 2. Under Connection -&gt; SSH -&gt; X11</p> <ol> <li>Select the option Enable X11 forwarding</li> </ol> <p> 3. Under Session</p> <ol> <li>Set Host Name (or IP address): <code>&lt;span&gt;</code>@pub.nrc-cnrc.gc.ca <code>&lt;span&gt;</code>@trixie.nrc-cnrc.gc.ca <li>Set Port: 22</li> <li>Add a name for Saved Sessions \u2013 perhaps Trixie</li> <p> 4. Click Save</p> <p>Once the settings have been saved, you can double click on the name in the list of Saved Sessions to open a session to the Trixie server. Please note that you will be prompted as follows</p> <ol> <li>LoginTC prompt \u2013 enter 1</li> <li>Prompt for your PUB password</li> <li>Prompt for your RES admin password</li> </ol>"},{"location":"External-Access-Advanced-Configuration/#related-topics","title":"Related Topics","text":"<p>External Access Setup</p> <p>File Transfers</p>"},{"location":"External-Access-Setup/","title":"Overview","text":"<p>As an external NRC collaborator, you can access the AI for Design (Trixie) Cluster using the Bastion Host. External collaborators include non-NRC researchers, industrial partners, and vendors.</p> <p>You can access only those folders on Trixie that are required for your project. Requests for access to Trixie and specific projects must be made by your NRC research contact; you cannot request access to a system yourself.</p> <p>Once granted access, you will have two sets of credentials issued to access the cluster:</p> Account Purpose User name format (example: John Doe) PUB Provides access to the external bastion host and used for theLoginTC second factor authentication A combination of your first and last name. E.g.: john.doe@pub.nrc-cnrc.gc.ca Trixie System Provides access to Trixie admin.firstname.lastname E.g.: admin.john.doe <p>Your NRC contact, or an NRC system administrator, will provide you with the PUB and Admin user names and passwords that you require to access the NRC systems. Note that on first login, you will be required to change your password. Please note: during the password change, the first prompt asks for a confirmation of your existing password prior to requesting a new one.</p>"},{"location":"External-Access-Setup/#logintc-application-setup","title":"LoginTC Application Setup","text":"<p>Before you attempt your first login, the following initial installation and configuration of LoginTC must be implemented.</p> <ul> <li>Upon user creation, you will receive an email to setup and initialize the LoginTC application (for iOS, Android, or the Chrome web browser) which is used as a second factor authentication into Trixie</li> <li>Set up LoginTC using the directions provided to you by email</li> </ul>"},{"location":"External-Access-Setup/#accessing-trixie-with-logintc-2-factor-authentication","title":"Accessing Trixie with LoginTC 2-Factor Authentication","text":"<p>In order to access Trixie, you will need to use an SSH client. Please note that you cannot access Trixie using a web browser. On Mac OSX and Linux, SSH is installed by default. On Windows you will need to install Putty if it is not installed already. You can download Putty from the following website:</p> <p>https://www.putty.org/</p>"},{"location":"External-Access-Setup/#initialize-ssh-connection-with-mac-osx-linux","title":"Initialize SSH Connection with Mac OSX / Linux","text":"<p>For Mac OSX and Linux you can open a new terminal and connect to <code>trixie.nrc-cnrc.gc.ca</code> via ssh using your PUB account and the following command</p> <p><code>ssh &lt;firstname.lastname&gt;@pub.nrc-cnrc.gc.ca trixie.nrc-cnrc.gc.ca</code></p>"},{"location":"External-Access-Setup/#initialize-ssh-connection-with-windows","title":"Initialize SSH Connection with Windows","text":"<p>For Windows, you can create a Putty profile to SSH into the bastion server</p> <p>Under Session</p> <ol> <li>Set Host Name (or IP address): <code>&lt;span&gt;</code>@pub.nrc-cnrc.gc.ca <code>&lt;span&gt;</code>@trixie.nrc-cnrc.gc.ca <li>Set Port: 22</li> <li>Add a name for Saved Sessions \u2013 perhaps Bastion</li> <p> 4. Click Save</p> <p>Once the settings have been saved, you can double click on the name in the list of Saved Sessions to open a session to the bastion server.</p>"},{"location":"External-Access-Setup/#logging-in-for-the-first-time","title":"Logging in for the First Time","text":"<p>When you login for the first time you will be forced to change your password for both your Pub account and your Trixie admin account. Please note that when you do this, you will be prompted for your original (or current) password first and then you will be prompted to enter your new password twice.</p> <p>In the following procedure, the information printed in the images may not be the same as what you will see when you login. However the steps will be the same.</p> <p>Please perform the following steps to access Trixie.</p> <ol> <li>When you login using one of the methods above, you will be prompted to authenticate with your LoginTC application. The message should appear as follows:</li> </ol> <p> 2. Press 1 followed by the Enter key and then check your LoginTC device as setup above to approve the login request 3. If a message similar to the one below appears, then simply type in yes to the prompt as shown below</p> <p> 4. After you complete the two-factor authentication process in LoginTC you will be prompted to enter your PUB account password and then you will be forced to change your password. You should see a message similar to the one below \u2013 remember to enter your original password first and then enter your new password twice.</p> <p> 5. The system will automatically log you out, thus, you will need to login again using your new password 6. Once you have successfully logged in, you will be logged into the bastion server \u2013 your screen should look similar to the following</p> <p> 7. If you have your credentials for the <code>trixie.res.nrc.gc.ca</code> server you can skip this step. Otherwise, you will now need to contact the administrator who provided you with your credentials for the bastion server to obtain your credentials for the Trixie server 8. You will need to login to Trixie next. From the bash prompt, use SSH to log into <code>trixie.res.nrc.gc.ca</code> with your Trixie admin. account and password with a similar command as the following. <p><code>ssh admin.&lt;firstname.lastname&gt;@trixie.res.nrc.gc.ca</code> 9. If a message similar to the one below appears, then simply type in yes to the prompt as shown below</p> <p> 10. You will be prompted to enter your Trixie admin account password and then you will be forced to change your password. You should see a message similar to the one below \u2013 remember to enter your original password first and then enter your new password twice.</p> <pre><code>![images/login3.png](images/login3.png)\n</code></pre> <ol> <li>The system will automatically log you out, thus, you will need to login again using your new password</li> <li> <p>Once you have successfully logged in, you will be logged into Trixie \u2013 your screen should look similar to the following</p> <p></p> </li> </ol> <p>After successful authentication, you should see the Trixie cluster login banner with terms and be placed in a shell in your home directory on the cluster, similar to the image above.</p> <p>Note that you will be placed in your home directory which only you have access to. For more information on the cluster and its usage, please see the:</p> <p>Home</p>"},{"location":"External-Access-Setup/#changing-passwords","title":"Changing passwords","text":"<p>Passwords on the PUB and RES accounts expire after 90 days and must be changed. If you do not change your password, you will be locked out of the system.</p> <p>Watch for the pop-up message notifying you to change your password, or set yourself a reminder to change your password before the 90-day expiry.</p> <p>If you get locked out of your account due to an expired password for any account, notify your NRC contact who can have the password reset.</p>"},{"location":"External-Access-Setup/#change-your-pub-password","title":"Change Your PUB Password","text":"<p>You can change your PUB password by logging into the following website. The site allows you to manage your PUB account. Please use the following format for your username <code>john.doe@pub</code></p> <p>PUB Account Management</p> <p>Please note that the Reset Password feature will not work if you do not fill in the security questions on the website. Therefore it is strongly recommended that you fill in the security questions so that you can reset your password if necessary.</p>"},{"location":"External-Access-Setup/#change-your-admin-password-via-linux-terminal","title":"Change Your Admin Password via Linux Terminal","text":"<ol> <li>Ensure you are logged into the Trixie server (trixie.res.nrc.gc.ca)</li> <li>Type passwd then hit Enter</li> <li>You will be prompted for your original (or current) password first and then you will be prompted to enter your new password twice. You should see a message similar to the one below \u2013 remember to enter your original password first and then enter your new password twice.</li> </ol> <p> 4. The system will automatically log you out, thus, you will need to login again using your new password</p>"},{"location":"External-Access-Setup/#related-topics","title":"Related Topics","text":"<p>External Access Advanced Configuration</p> <p>File Transfers</p>"},{"location":"External-HPC-Systems/","title":"Overview","text":"<p>There may be instances where researchers require connectivity to external HPC systems from Trixie. However, network access to and from Trixie is restricted to maintain a high level of security. Therefore, connections to external systems need to be approved before the connection can be opened.</p> <p>This page provides instructions for requesting a connection to an external system, as well as a list of approved systems that already have an open connection.</p>"},{"location":"External-HPC-Systems/#request-a-connection-to-an-external-system","title":"Request a Connection to an External System","text":"<p>In order to submit a request to open a network flow between Trixie and an external HPC system, please post your request in the issues section of this site.</p>"},{"location":"External-HPC-Systems/#approved-external-systems","title":"Approved External Systems","text":"Institution System URL Compute Canada - Cedar cedar.computecanada.ca Compute Canada - Beluga beluga.computecanada.ca Compute Canada - Niagra niagra.computecanada.ca Compute Canada - Graham graham.computecanada.ca Vector Institute v.vectorinstitute.ca NERSC.gov - Cori cori.nersc.gov"},{"location":"File-Transfers/","title":"Overview","text":"<p>This document will describe various procedures for transferring files to and from Trixie.</p> <p>Important Note: For external users, before proceeding with this configuration, please ensure that you have performed the external access setup and advanced configuration procedures.</p>"},{"location":"File-Transfers/#transfers-between-your-local-computer-and-trixie","title":"Transfers Between Your Local Computer and Trixie","text":"<p>The following sections detail how to transfer files between your local computer and Trixie. They basically rely on advanced SSH configurations to bridge the network between your local computer and Trixie.</p>"},{"location":"File-Transfers/#mac-osx-linux","title":"Mac OSX / Linux","text":"<p>To copy a file to the Trixie server, please use the scp command on your local machine.</p>"},{"location":"File-Transfers/#external-users","title":"External Users","text":"<p>Please note that the use of this method requires that your system be configured as detailed in the advanced configuration in order to provide a direct link between your local machine and the Trixie server.</p> <p>The following command will copy the file <code>test.txt</code> from John Doe\u2019s local machine to his admin.john.doe account on Trixie. Please note that using trixie as the hostname will only work if you have configured SSH to use ProxyJump as detailed in the advanced configuration.</p> <p><code>scp test.txt trixie:/home/admin.john.doe</code></p> <p>To copy a file from Trixie to your local machine, you basically reverse the arguments to the scp command.</p> <p><code>scp trixie:/home/admin.john.doe/test.txt test.txt</code></p> <p>To copy an entire directory instead of just a file, please use the \u2013r option (for recursive) to the scp command.</p> <p><code>scp \u2013r myWorkFilesDir trixie:/home/admin.john.doe</code></p>"},{"location":"File-Transfers/#internal-users","title":"Internal Users","text":"<p>The following command will copy the file <code>test.txt</code> from John Doe\u2019s local machine to his account on Trixie. Please note that the example assumes the username on Trixie is different than the username on the local machine.</p> <p><code>scp test.txt doej@trixie.res.nrc.gc.ca:/home/doej</code></p> <p>To copy a file from Trixie to your local machine, you basically reverse the arguments to the scp command.</p> <p><code>scp doej@trixie.res.nrc.gc.ca:/home/doej/test.txt test.txt</code></p> <p>To copy an entire directory instead of just a file, please use the \u2013r option (for recursive) to the scp command.</p> <p><code>scp \u2013r myWorkFilesDir doej@trixie.res.nrc.gc.ca:/home/doej</code></p>"},{"location":"File-Transfers/#windows-using-winscp","title":"Windows Using WinSCP","text":"<p>To copy a file to the Trixie server, please use the WinSCP command on your local machine.</p>"},{"location":"File-Transfers/#external-users_1","title":"External Users","text":"<p>If you need to install WinSCP then please download and install it from this site</p> <p>First you will need to configure WinSCP to connect to Trixie using an SSH tunnel. Open WinSCP and follow the procedure below to configure it to access Trixie via an SSH tunnel.</p> <ol> <li>Click the New Session button</li> </ol> <p> 2. In the window that pops up, perform the following</p> <ol> <li>Make sure the File protocol is set to SCP</li> <li>Set the Host name: trixie.res.nrc.gc.ca</li> <li> <p>Set the User name: \\<code>&lt;br&gt;</code>       The window should now look similar to the following <p>    4. Click the Advanced button 3. In the window that pops up, perform the following</p> <li> <p>Click the Tunnel item in the left pane</p> </li> <li>Select the Connect through SSH tunnel option</li> <li>Set Host name: trixie.nrc-cnrc.gc.ca</li> <li> <p>Set User name: \\<code>&lt;span&gt;</code>@pub.nrc-cnrc.gc.ca <code>&lt;br&gt;</code>       The window should now look similar to the following <p>    5. Click the OK button 4. Click the Save button in the previous popup window 5. In the window that pops up, perform the following</p> <li> <p>Type in a Site name - perhaps Trixie <code>&lt;br&gt;</code>       The window should now look similar to the following</p> <p>    2. Click the OK button 6. Click the Login button in the previous popup window <code>&lt;br&gt;</code>    You will be prompted to authenticate with LoginTC (you will need to type 1) and both your Pub and Trixie passwords 7. Once you are logged into your session, you can drag and drop the files you need to transfer between the two file listings</p> </li>"},{"location":"File-Transfers/#internal-users_1","title":"Internal Users","text":"<p>If you need to install WinSCP then please install it from the NRC Software Portal on your desktop.</p> <p>First you will need to configure WinSCP to connect to Trixie. Open WinSCP and follow the procedure below to configure it to access Trixie.</p> <ol> <li>Click the New Session button</li> </ol> <p> 2. In the window that pops up, perform the following</p> <ol> <li>Make sure the File protocol is set to SCP</li> <li>Set the Host name: trixie.res.nrc.gc.ca</li> <li> <p>Set the User name: \\<code>&lt;br&gt;</code>       The window should now look similar to the following <p>    4. Click the Save button 3. In the window that pops up, perform the following</p> <li> <p>Type in a Site name - perhaps Trixie <code>&lt;br&gt;</code>       The window should now look similar to the following</p> <p>    2. Click the OK button 4. Click the Login button in the previous popup window <code>&lt;br&gt;</code>    You will be prompted to authenticate with your Trixie password 5. Once you are logged into your session, you can drag and drop the files you need to transfer between the two file listings</p> </li>"},{"location":"File-Transfers/#windows-using-the-pscp-command-from-putty","title":"Windows Using the pscp Command From Putty","text":"<p>To copy a file to the Trixie server, please use the pscp command on your local machine.</p>"},{"location":"File-Transfers/#external-users-this-process-does-not-work-at-the-moment","title":"External Users - this process does not work at the moment","text":"<p>Please note that the use of this method requires that you have two Putty profiles defined.</p> <ol> <li>A profile for the bastion server</li> <li>A profile for the Trixie server</li> </ol>"},{"location":"File-Transfers/#bastion-server-profile","title":"Bastion Server Profile","text":"<p>The bastion server profile was likely created during the setup configuration for your external access to Trixie. If not, then please see the initialize SSH connection section for detailed instructions on creating a profile for the bastion server.</p>"},{"location":"File-Transfers/#trixie-server-profile","title":"Trixie Server Profile","text":"<p>Follow the procedure below to create the Trixie server profile.</p> <p>Under Session</p> <ol> <li>Set Host Name (or IP address): \\<code>&lt;span&gt;</code>@trixie.res.nrc.gc.ca <li>Set Port: 22</li> <li>Add a name for Saved Sessions \u2013 perhaps Trixie-pscp</li> <p> 4. Click Save</p> <p>Once you have the profiles created and saved, please follow the procedure below to run the pscp command.</p> <ol> <li>Load the Bastion profile and click Open</li> <li>Login to the bastion server and leave the window open</li> <li>Open a Command Prompt window</li> <li> <p>Use the pscp command in the Command Prompt window to copy files to or from the trixie server using the Trixie-pscp putty profile</p> </li> <li> <p>Copy the file <code>test.txt</code> from John Doe\u2019s local machine to his admin.john.doe account on trixie</p> <p><code>pscp test.txt Trixie-pscp:/home/admin.john.doe</code>    2. To copy a file from trixie to your local machine, you basically reverse the arguments to the pscp command</p> <p><code>pscp Trixie-pscp:/home/admin.john.doe/test.txt test.txt</code>    3. To copy an entire directory instead of just a file, please use the \u2013r option (for recursive) to the pscp command</p> <p><code>pscp \u2013r myWorkFilesDir Trixie-pscp:/home/admin.john.doe</code></p> </li> </ol>"},{"location":"File-Transfers/#internal-users_2","title":"Internal Users","text":"<p>Please note that the use of this method requires that you have a Putty profile defined to access the Trixie server. Follow the procedure below to create the Trixie server profile.</p> <p>Under Session</p> <ol> <li>Set Host Name (or IP address): \\<code>&lt;span&gt;</code>@trixie.res.nrc.gc.ca <li>Set Port: 22</li> <li>Add a name for Saved Sessions \u2013 perhaps Trixie-pscp</li> <p> 4. Click Save</p> <p>Once you have the profile created and saved, please follow the procedure below to run the pscp command.</p> <ol> <li>Open a Command Prompt window</li> <li> <p>Use the pscp command in the Command Prompt window to copy files to or from the trixie server using the Trixie-pscp putty profile</p> </li> <li> <p>Copy the file <code>test.txt</code> from John Doe\u2019s local machine to his doej account on trixie</p> <p><code>pscp test.txt Trixie-pscp:/home/doej</code>    2. To copy a file from trixie to your local machine, you basically reverse the arguments to the pscp command</p> <p><code>pscp Trixie-pscp:/home/doej/test.txt test.txt</code>    3. To copy an entire directory instead of just a file, please use the \u2013r option (for recursive) to the pscp command</p> <p><code>pscp \u2013r myWorkFilesDir Trixie-pscp:/home/doej</code></p> </li> </ol>"},{"location":"File-Transfers/#transfers-between-trixie-and-another-hpc-cluster","title":"Transfers Between Trixie and Another HPC Cluster","text":""},{"location":"File-Transfers/#needs-verification-that-this-is-accurate-and-works-as-well-as-real-parameters-for-the-command-lines-please","title":"Needs verification that this is accurate and works, as well as real parameters for the command lines please","text":"<p>The procedures in this section assume that the advanced SSH configurations discussed above have been implemented. There are three options for copying files between Trixie and another HPC cluster</p> <ol> <li>Copy files directly between Trixie and the HPC cluster</li> <li>Login to Trixie from the other HPC cluster</li> <li>Copy files through your local computer</li> </ol>"},{"location":"File-Transfers/#copy-files-directly","title":"Copy Files Directly","text":"<p>This procedure requires that there is an approved network flow open between Trixie and the second HPC cluster. Please see the external HPC systems page for a list of approved external HPC systems. If there is an approved network flow, then files can be directly copied between Trixie and the second HPC cluster. This is the ideal situation and should be the fastest option in terms of overall network speed between the two systems.</p> <p>To copy a file from the second HPC cluster to Trixie, use the following scp command on the Trixie server.</p> <p><code>scp username@cluster.domain:/home/username/test.txt test.txt</code></p> <p>To copy a file from Trixie to the second HPC cluster, you basically reverse the arguments to the scp command.</p> <p><code>scp test.txt username@cluster.domain:/home/username/test.txt</code></p> <p>To copy an entire directory instead of just a file, please use the \u2013r option (for recursive) to the scp command.</p> <p><code>scp \u2013r myWorkFilesDir username@cluster.domain:/home/username/folder</code></p>"},{"location":"File-Transfers/#login-to-trixie-from-second-cluster","title":"Login to Trixie From Second Cluster","text":"<p>This procedure requires that you have an external account setup to access Trixie. If this is the case, then files can be copied between Trixie and the second HPC cluster via the Bastion Host, but without flowing through your local computer. To use this approach, you will need to login to the second HPC cluster first, and then from the second HPC cluster computer, login to Trixie through the Bastion host.</p> <p>To copy a file from the second HPC cluster to Trixie, use the following scp command on the Trixie server.</p> <p><code>scp username@cluster.domain:/home/username/test.txt test.txt</code></p> <p>To copy a file from Trixie to the second HPC cluster, you basically reverse the arguments to the scp command.</p> <p><code>scp test.txt username@cluster.domain:/home/username/test.txt</code></p> <p>To copy an entire directory instead of just a file, please use the \u2013r option (for recursive) to the scp command.</p> <p><code>scp \u2013r myWorkFilesDir username@cluster.domain:/home/username/folder</code></p>"},{"location":"File-Transfers/#copy-files-through-your-local-computer","title":"Copy Files Through Your Local Computer","text":"<p>This procedure requires that you copy files between the two clusters using your local computer as a bridge. The commands below should be executed on your local computer and not either of the cluster servers.</p> <p>To copy a file from the second HPC cluster to Trixie, use the following scp command on your local computer.</p> <p><code>scp username@cluster.domain:/home/username/test.txt trixie:/home/admin.john.doe/test.txt</code></p> <p>To copy a file from Trixie to the second HPC cluster, you basically reverse the arguments to the scp command.</p> <p><code>scp trixie:/home/admion.john.doe/test.txt username@cluster.domain:/home/username/test.txt</code></p> <p>To copy an entire directory instead of just a file, please use the \u2013r option (for recursive) to the scp command.</p> <p><code>scp \u2013r trixie:/home/admin.john.doe/myWorkFilesDir username@cluster.domain:/home/username/folder</code></p>"},{"location":"File-Transfers/#copy-files-to-a-project-folder","title":"Copy Files to a Project Folder","text":"<p>Project folders have been created for users to use for a couple of purposes:</p> <ol> <li>Storage of data files to use with Trixie. Although you can use your home directory for limited storage of files, it is strongly recommended that you use the project folder instead as there are higher disk quotas for project folders.</li> <li>Sharing of project work and files with team members</li> </ol> <p>Please note that users should be diligent and remove any files and folders (in both the project folder and your home folder) once they are no longer required. This helps to optimize disk usage and avoid disk space issues for all users, not just your own usage.</p> <p>The project folder can be found under the following folder hierarchy</p> <p><code>/gpfs/projects/&lt;project-group&gt;/&lt;project&gt;</code></p> <p>Where project-group is the name of your project group \u2013 for example, AI4D or COVID - and project is the name of your project \u2013 for example, core-01 or bio-01.</p> <p>To copy files to a project folder you should create a personal folder under the project directory and then copy files from your home directory to the new folder. In the example below user John Doe will copy two dataset files to the AI4D/bio-01 project folder.</p> <ol> <li>Change directory to the project folder</li> </ol> <p><code>cd /gpfs/projects/AI4D/bio-01</code> 2. Create the new folder using a unique name, perhaps your last name and first initial</p> <p><code>mkdir doej</code> 3. Change back to your home directory</p> <p><code>cd</code> 4. Copy the files to your new project directory</p> <p><code>cp dataset1.dat dataset2.dat /gpfs/projects/AI4D/bio-01/doej</code></p>"},{"location":"File-Transfers/#related-topics","title":"Related Topics","text":"<p>External Access Setup</p> <p>External Access Advanced Configuration</p>"},{"location":"Hardware/","title":"Hardware","text":"<p>Trixie is a GPU cluster consisting of 36 nodes, each with NVIDIA V100 GPU, a fast, Infiniband Interconnect, and a large 1 PB global filesystem</p>"},{"location":"Hardware/#operating-system","title":"Operating system","text":"<p>Runs RHEL 9</p>"},{"location":"Hardware/#job-scheduler","title":"Job scheduler","text":"<p>https://slurm.schedmd.com <code>slurm 22.05.9</code> (for example run scripts on Trixie see Running-jobs)</p>"},{"location":"Hardware/#headnode-2-available","title":"Headnode (2 available)","text":"<ul> <li>processor_type = Intel Xeon Gold 6130 CPU clocked at 2.1GHZ 16 cores / CPU</li> <li>processors_per_node = 2</li> <li>RAM = 96 GB memory</li> </ul>"},{"location":"Hardware/#node-profile","title":"Node Profile","text":"<ul> <li>processor_type = Intel Xeon Gold 6130 CPU clocked at 2.1GHZ 16 cores / CPU</li> <li>processors_per_node = 2</li> <li>cores_per_socket = 16</li> <li>threads_per_core = 2 (hyper-threading on)</li> <li>RAM = 192 GB memory</li> <li>GPU_type = NVidia V100 GPUs with 32 GB RAM / GPU</li> <li>GPU_details = https://www.nvidia.com/en-us/data-center/tesla-v100/Nodes</li> <li>GPU_per_node = 4</li> <li>local scratch size =</li> </ul>"},{"location":"Hardware/#filesystem","title":"Filesystem","text":"<ul> <li>1 PB of GPFS storage total, with the following mount points and quotas</li> <li>/home/usernameUser space. Not backed up. Soft quota 1 TB.</li> <li>/gpfs/projects/PROGRAM/project_idShared space for sharing files between users within a project. Each project has a default quota of 1 TB. If additional space is needed   projects can make an official request. Space is not backed up.</li> <li>/gpfs/scratch/   User controlled scratch space for running jobs. Not backed up. Subject to Purge policy (LINK LOST)</li> </ul>"},{"location":"Jobs-conda-pytorch/","title":"conda-pytorch","text":"<p>This examples will show you how to setup and prepare an environment for PyTorch jobs using conda on Trixie:</p>"},{"location":"Jobs-conda-pytorch/#1-create-a-pytorch-miniconda-environment","title":"1. Create a pytorch miniconda environment:","text":"<p>Either run from the command line or create pytorchconda-environment.sh and run it:</p> <pre><code>#!/bin/bash\n# load the miniconda module\nmodule load miniconda3-4.8.2-gcc-9.2.0-sbqd2xu\n# create a conda environment with python 3.7 named pytorch\nconda create --name pytorch python=3.7\nsource activate pytorch\n# install pytorch dependencies via conda\nconda install pytorch==1.7.1 torchvision==0.8.2 cudatoolkit=10.1 -c pytorch\n</code></pre>"},{"location":"Jobs-conda-pytorch/#2-create-a-test-pytorch-python-script-testtorchpy","title":"2. Create a test pytorch python script: testtorch.py","text":"<pre><code>import torch\nprint('GPU available:', torch.cuda.is_available())\n</code></pre>"},{"location":"Jobs-conda-pytorch/#3-create-a-job-submission-script-testpytorchsh","title":"3. Create a job submission script: testpytorch.sh","text":"<pre><code>#!/bin/bash\n\n# Specify the partition of the cluster to run on (Typically TrixieMain)\n#SBATCH --partition=TrixieMain\n# Add your project account code using -A or --account\n#SBATCH --account ai4d\n# Specify the time allocated to the job. Max 12 hours on TrixieMain queue.\n#SBATCH --time=12:00:00\n# Request GPUs for the job. In this case 4 GPUs\n#SBATCH --gres=gpu:4\n# Print out the hostname that the jobs is running on\nhostname\n# Run nvidia-smi to ensure that the job sees the GPUs\n/usr/bin/nvidia-smi\n\n# Load the miniconda module on the compute node\nmodule load miniconda3-4.8.2-gcc-9.2.0-sbqd2xu\n# Activate the conda pytorch environment created in step 1\nsource activate pytorch\n# Launch our test pytorch python file\npython testtorch.py\n</code></pre>"},{"location":"Jobs-conda-pytorch/#4-submit-job-for-execution","title":"4. Submit job for execution:","text":"<pre><code>sbatch testpytorch.sh\n</code></pre> <p>Output will be 'Submitted batch job XXXXX'</p>"},{"location":"Jobs-conda-pytorch/#5-confirm-execution-results","title":"5. Confirm execution results:","text":"<p>Local directory will contain a file 'slurm-XXXXX.out' which is the output of the job (stdout).</p> <p>Output should be:</p> <pre><code>cnXXX - &lt;nodename&gt;\n&lt;Date&gt;\n+--------\n| NVIDIA-SMI XXXX...\n....\n(4 listed V100 GPUs number 0 to 3)\n\nGPU available: True\n</code></pre>"},{"location":"Networking-and-connectivity/","title":"Network and Connection","text":"<p>The Trixie head node can be accessed via ssh on NRC Black &amp; NRC Orange</p> <p>The Trixie head node has outbound ssh access to a limited number of external sites (e.g. some Canadian Universities). If you require access to an additional site which is not currently available, create a request via https://github.com/ai4d-iasc/trixie/issues</p>"},{"location":"Running-jobs/","title":"Quickstart","text":"<p>Trixie use the slurm scheduler to manage jobs. Compute Canada has a very good guide for using slurm to submit jobs to a cluster most of which is applicable for Trixie: https://docs.computecanada.ca/wiki/Running_jobs</p> <p>Here is a simple job which runs the python code hello.py</p> <p>Contents of hello.py</p> <pre><code>print('Hello world')\n</code></pre> <p>Contents of hello-job.sh</p> <pre><code>#!/bin/bash\n#SBATCH -J helloworld\n\nmodule load miniconda3-4.8.2-gcc-9.2.0-sbqd2xu\nsrun python ~/hello.py\n</code></pre> <p>Submit job:</p> <pre><code>sbatch ./hello-job.sh\n</code></pre> <p>Output will be located in slurm-<code>&lt;jobid&gt;</code>.out</p> <p>In order for a job to run on Trixie, it must be \"billed\" against an approved project. Users are able to charge different projects depending on what their activity is for.</p> <p>See here for the Account Codes</p>"},{"location":"Running-jobs/#more-jobs-examples","title":"More jobs examples:","text":"<ul> <li>jobs-conda-pytorch</li> <li>jobs-conda-jupyterlab</li> <li>jobs-conda-RAPIDS</li> <li>jobs-abinit</li> </ul>"},{"location":"SLURM%2C-pytorch-distributed-and-Multiple-Nodes/","title":"Running pytorch.distributed on Multiple Nodes.","text":"<p>Key thing to know is that srun is like a super-ssh which means that when running <code>srun cmd</code> it actually does something like <code>ssh node cmd</code></p>"},{"location":"SLURM%2C-pytorch-distributed-and-Multiple-Nodes/#taskslurm","title":"task.slurm","text":"<pre><code>#!/bin/bash\n\n#SBATCH --partition=TrixieMain\n#SBATCH --account=dt-mtp\n#SBATCH --time=00:20:00\n#SBATCH --job-name=pytorch.distributed\n#SBATCH --comment=\"Helping Harry with pytorch distributed on multiple nodes.\"\n#SBATCH --gres=gpu:4\n##SBATCH --ntasks=2\n\n#SBATCH --wait-all-nodes=1\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=6\n#SBATCH --exclusive\n#SBATCH --output=%x-%j.out\n\n\n# USEFUL Bookmarks\n# [Run PyTorch Data Parallel training on ParallelCluster](https://www.hpcworkshops.com/08-ml-on-parallelcluster/03-distributed-data-parallel.html)\n# [slurm SBATCH - Multiple Nodes, Same SLURMD_NODENAME](https://stackoverflow.com/a/51356947)\n\nreadonly MASTER_ADDR_JOB=$SLURMD_NODENAME\nreadonly MASTER_PORT_JOB=\"12234\"\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\nreadonly srun='srun --output=%x-%j.%t.out'\n\nenv\n\n$srun bash \\\n   task.sh \\\n      $MASTER_ADDR_JOB \\\n      $MASTER_PORT_JOB &amp;\n\nwait\n</code></pre>"},{"location":"SLURM%2C-pytorch-distributed-and-Multiple-Nodes/#tasksh","title":"task.sh","text":"<p>This script will be executed on each node. Note that we are activating the <code>conda</code> environment in this script so that each node/worker can have the proper environment.</p> <pre><code>#!/bin/bash\n\n# USEFUL Bookmarks\n# [Run PyTorch Data Parallel training on ParallelCluster](https://www.hpcworkshops.com/08-ml-on-parallelcluster/03-distributed-data-parallel.html)\n# [slurm SBATCH - Multiple Nodes, Same SLURMD_NODENAME](https://stackoverflow.com/a/51356947)\n\n#module load miniconda3-4.7.12.1-gcc-9.2.0-j2idqxp\n#source activate molecule\n\nsource /gpfs/projects/DT/mtp/WMT20/opt/miniconda3/bin/activate\nconda activate pytorch-1.7.1\n\nreadonly MASTER_ADDR_JOB=$1\nreadonly MASTER_PORT_JOB=$2\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\nenv\n\npython \\\n   -m torch.distributed.launch \\\n      --nproc_per_node=4 \\\n      --nnodes=$SLURM_NTASKS \\\n      --node_rank=$SLURM_NODEID \\\n      --master_addr=$MASTER_ADDR_JOB \\\n      --master_port=$MASTER_PORT_JOB \\\n      main.py \\\n         --batch_size 128 \\\n         --learning_rate 5e-5 &amp;\n\nwait\n</code></pre>"},{"location":"Temporary-Filesystem-Backups/","title":"Overview","text":"<p>It is possible to retrieve data from automatically generated filesystem backups. The IBM Spectrum Scale (GPFS) system includes the ability to create filesystem snapshots which create temporary backups of data stored in the filesystem. These snapshots can be accessed to retrieve files that may have been accidentally removed.</p> <p>Please see the GPFS Snapshot document for more details on how to retrieve files from the backups.</p>"},{"location":"Trixie-Status/","title":"Current Trixie Operational Status","text":""},{"location":"Trixie-Status/#upcoming-planned-downtime","title":"Upcoming Planned Downtime","text":"<ul> <li> <p>Thursday, October 17, 2024 - The Trixie cluster will be shutdown because of a planned electrical outage that will allow RPPM to    commission the new emergency power generator.</p> </li> <li> <p>Start date: Thursday, October 17, 6:00 AM EDT</p> </li> <li>End date: Monday, October 21, 6:00 PM EDT</li> </ul> <p>If you have any further questions, do not hesitate to contact us at your earliest convenience (rps-spr@nrc-cnrc.gc.ca).</p> <ul> <li> <p>Tuesday, October 22nd, 2024 - As a reminder, LoginTC is used as a second authentication service for:</p> </li> <li> <p>Bastion host for external access to Trixie High Performance Computing Clusters</p> </li> <li>NRC external collab</li> <li>NetOps</li> </ul> <p>A maintenance period is required to perform system upgrades. Therefore, the LoginTC service will be unavailable on Tuesday, October 22nd, from 3PM to 5PM EDT</p> <p>Consequently, you will not be able to access the service for which LoginTC provides authentication.</p> <p>Internal access to Trixie will still be available during this time.</p> <p>If you have any questions regarding this maintenance, do not hesitate to communicate with us (rps-spr@nrc-cnrc.gc.ca)</p>"},{"location":"Trixie-Status/#current-issues-outages","title":"Current Issues / Outages","text":""},{"location":"Trixie-Status/#none","title":"None","text":""},{"location":"Trixie-Status/#past-events-incidents","title":"Past Events / Incidents","text":"<ul> <li>[RESOLVED] - Friday September 20, 2024 - Yesterday evening, a Centrify outage affected the users ability to connect the to Hartree and Trixie clusters. Opened sessions at the time are also affected and are now stale and should be ended. Currently: Users trying to connect are still experiencing abnormal behaviors. We are still investigating whether the jobs that were running at the time were affected by the outage. KITS is working on fixing the issue and we will inform you as the situation evolves.</li> <li>[RESOLVED] - Friday June 28, 2024 - The Research Platform Support team is currently proceeding with operating system and storage appliance upgrades on the cluster. In order to do the storage appliance upgrade, the cluster will be offline from the 28th of June at 6AM EDT to the afternoon of July the 2nd for a final data synchronization from the old appliance to the new appliance. Please see the email sent out to users Friday, June 21 for important details concerning this change in the Trixie infrastructure. Thank you for your patience. Research Platform Support - Update - Due to ongoing RES VPN issues affecting the upgrade of the Trixie cluster, the return to service has been delayed to end-of-business July 3rd, 2024. - Update - Due to further RES VPN issues today affecting the upgrade of the Trixie cluster, the return to service has been delayed to July 4th, 2024. A notice will be sent when Trixie is back online.</li> <li>[RESOLVED] - Thursday, June 6, 2024 - Trixie Bastion Host Shutdown Notice - For users connecting from the Internet or the Legacy network. This bastion host upgrades will require downtime that is scheduled to start at 7:00AM EDT on June 6th and will conclude at 5PM. Although the bastion hosts will be offline during this time, jobs summitted prior to this maintenance window by users connecting through the bastion host will continue to run normally. If you have any questions or concerns about this upgrade, please let us know. Thank you for your patience. Research Platform Support</li> <li>[RESOLVED] - Tuesday, June 5, 2024 - There is currently a firewall issue resolving some addresses/URLs from the Digital Research Alliance of Canada (formerly Compute Canada) CVMFS mirrors which is affecting the loading of some modules. Please report any outstanding issues on either the issues page or the RPS mailbox.</li> <li>[RESOLVED] - Friday, May 3, 2024 - This downtime is scheduled to start at 2:30PM EDT on Friday May 3rd and will conclude on the evening of Sunday the 5th. A notice will be sent out when the downtime is completed, and the cluster is back online. If you have any questions or concerns about this shutdown, please let us know. Thank you for your patience. Research Platform Support</li> <li>[RESOLVED] - Friday, April 12, 2024 - This downtime is scheduled to start at 2:30PM EDT on Friday April 12th and will conclude on the evening of Sunday the 14th. A notice will be sent out when the downtime is completed, and the cluster is back online. If you have any questions or concerns about this shutdown, please let us know. Thank you for your patience. Research Platform Support</li> <li>[RESOLVED] - Friday, February 2, 2024 - This downtime is scheduled to start at 2:00PM EST on February 2nd and will conclude on the evening of the 3rd. A notice will be sent out when the downtime is completed, and the cluster is back online.  If you have any questions or concerns about this shutdown, please let us know. Thank you for your patience. Research Platform Support</li> <li>[RESOLVED] - Friday, January 19, 2023 - This downtime is scheduled to start at 2:30 PM EST on Friday January 19th and will conclude on the evening of the 20th. A notice will be sent out when the downtime is completed, and the cluster is back online. If you have any questions or concerns about this shutdown, please let us know. Thank you for your patience. Research Platform Support</li> <li>[CANCELLED] - Thursday, December 14th, 2023 - This downtime is scheduled to start at 2:30 PM EST on Thursday the 14th of December and will conclude on the morning of the 15th. A notice will be sent out when the downtime is completed, and the cluster is back online. If you have any questions or concerns about this shutdown, please let us know. Thank you for your patience. Research Platform Support</li> <li>[RESOLVED] - Monday, August 28th, 2023 - KITS will be performing a maintenance on the Trixie HPC cluster on Tuesday, August 29th at 6AM EDT. The cluster should be brought back online around 12PM. Jobs with a run time conflicting with the maintenance starting period will stay in the queue and run after the maintenance. A notice will be sent out when the downtime is completed and the cluster is back online.  If you have any questions or concerns please let us know. Research Platform Support: rps-spr@nrc-cnrc.gc.ca</li> <li>[RESOLVED] - Wednesday July 19, 2023 - Thursday July 20, 2023 - Please note that RPPM will be shutting down regular power to building M-55 for electrical emergency repairs on July 20th from 6 to 7AM EDT. KITS will therefore be shutting down the Trixie HPC from July 19th at 5 PM to soon after 7AM on the 20th. A notice will be sent out when the downtime is completed and the cluster is back online. If you have any questions or concerns please let us know. IT Operations: ITOperations-OperationsTI@nrc-cnrc.gc.ca</li> <li>[RESOLVED] - Monday June 19, 2023 - Wednesday June 21, 2023 - Please note that in support of the new generator installation taking place at building M-55, RPPM will be shutting down power to building M-55, taking place on two consecutive evenings. Trixie HPC will be unavailable during the scheduled period of Monday June 19th 1:00 pm EDT to morning of Wednesday June 21st. A notice will be sent out when the downtime is completed, and the cluster is back online. If you have any questions or concerns about this maintenance or the maintenance schedule please let us know. Thank you for your patience. IT Operations: ITOperations-OperationsTI@nrc-cnrc.gc.ca</li> <li>[RESOLVED] - Friday June 9, 2023 - Monday June 12, 2023 - A period of downtime is required for the Trixie HPC due to work on the buildings electrical systems. We will also use this time to perform some routine maintenance and upgrades. This downtime is scheduled to start at 2:00 PM EDT on Friday June 9th and will conclude on the afternoon of Monday June 12th. A notice will be sent out when the downtime is completed, and the cluster is back online. If you have any questions or concerns about this maintenance or the maintenance schedule please let us know. Thank you for your patience. IT Operations: ITOperations-OperationsTI@nrc-cnrc.gc.ca</li> <li>[RESOLVED] - Monday March 27, 2023 - A period of downtime is required for the Trixie HPC cluster to perform some routine maintenance and upgrades. This downtime is scheduled for the day of Monday March 27th. A notice will be sent out when the downtime is completed, and the cluster is back online. If you have any questions or concerns about this maintenance or the maintenance schedule please let us know. Please note that Slurm should stop accepting jobs that would run into the maintenance period. They will be held in the queue until the maintenance period has ended. Thank you for your patience. IT Operations: ITOperations-OperationsTI@nrc-cnrc.gc.ca</li> <li>[RESOLVED] - Monday November 28, 2022 - A period of downtime is required for the Trixie HPC cluster to perform some routine maintenance and upgrades. This downtime is scheduled for the day of Monday November 28th. A notice will be sent out when the downtime is completed, and the cluster is back online. If you have any questions or concerns about this maintenance or the maintenance schedule please let us know. Thank you for your patience. IT Operations: ITOperations-OperationsTI@nrc-cnrc.gc.ca</li> <li>[RESOLVED] - Monday May 30, 2022 - Please note that the Trixie server is currently offline - possibly due to a network issue.</li> <li>[RESOLVED] - Monday April 11 - Wednesday April 13, 2022 (3 days) Please note that Trixie (AI4D HPC cluster) will be unavailable from April 11-13th (3 days) due to a scheduled upgrade. The GPFS file system will be upgraded during this time. If you have any questions or concerns about this maintenance please send an email to ITOperations-OperationsTI@nrc-cnrc.gc.ca. Update - Thursday April 14, 2022: Due to complications with the upgrade of the storage array the scheduled downtime for the AI4D-Trixie cluster has been extended.  There is currently no estimate for when the cluster will return to service but an update will be sent out as soon as there is more information. - Update - Tuesday April 19, 2022 We are returning the AI4D-Trixie cluster to operational status. Unfortunately the storage array is in a degraded state and is only operating with 25% of its normal transfer capacity.  Expect to see slowness from i/o intensive operations. All of the Compute Nodes as well as the Head Node have been re-imaged.  The default operating environment has changed so expect many versions of the software loaded when you login to have changed. This may cause issues with job scripts created for the previous environment. If you experience any issues please let us know (ITOperations-OperationsTI@nrc-cnrc.gc.ca).</li> <li>[RESOLVED] - Tuesday January 25, 2022 - There appears to be several issues with the Trixie HPC that are impacting access through the Bastion Host and general performance on the headnode.  KITS-ITOps is investigating and hopes to resolve the issues as soon as possible.  We will provide an update when we have more information. Update - There is a technical issue with the SSC managed switch due to a recent power outage. Access from Legacy and RES should still function but external access through the Bastion Host is not working. An SSC technician is supposed to be on site tomorrow morning to investigate.</li> <li>[RESOLVED] - Wednesday December 15, 2021 - External access to Trixie is not available. It appears that there is an error with the SSL cert for the external LoginTC URL. When trying to use the LoginTC app on your phone to accept a login request a certificate error appears and the request is never received. Hopefully the issue will be resolved quickly, but access could be offline for a day or two. </li> <li>[RESOLVED] - Wednesday December 15, 2021 - Trixie is currently unavailable and the issue is being investigated.</li> <li>[RESOLVED] - Thursday Dec 2, 2021 - SSH connection to Trixie via the external bastion host are being blocked. Internal NRC network connectivity and Trixie operations continue normally. Investigation of root cause underway.</li> <li>[RESOLVED - downtime completed successfully] - Monday August 23, 2021 - There will be a maintenance period for the Trixe AI4D Cluster on Monday August 23rd starting at 8:00 am EDT.  Access to the cluster will not be possible during the maintenance. The entire day will be reserved for the maintenance but current estimates suggest it will be returned to service by noon.  Maintenance will involve the replacement of a power distribution unit in one of the racks as well as configuration changes on the primary head node. Every effort will be made to preserve the job queue during the maintenance.</li> <li>[RESOLVED - downtime completed successfully] - We are planning a period of scheduled downtime for the Trixie-AI4D cluster on Monday June 28th from 8:00am to 4:00pm EDT.  This will allow a few maintenance tasks to be performed that would interrupt service. These tasks include modifying the partition structure on the primary head node as well as some security patching. - It has become necessary to add a firmware update to this maintenance window for the Mellanox switches. This will cause the GPFS file system to become unavailable forcing us to shutdown the cluster entirely.  All jobs in the queue at the start of the maintenance period will likely be lost. - Due to unforeseen complications the maintenance period must be extended.</li> <li>[RESOLVED - downtime completed successfully] - A period of downtime for the Trixie (AI4D) cluster is being scheduled for Monday, May 17th from 8:00 am - 6:00 pm EDT. Due to a hardware issue on the storage array there will need to be a scheduled maintenance period as per the vendors recommendation. Please note that the nature of the maintenance will require all jobs in the queue to be terminated at the start of the maintenance window.  </li> <li>[RESOLVED - nodes back in main queue] - Compute nodes cn110 and cn125 have been taken out of the main queue to troubleshoot GPU issues</li> <li>[RESOLVED - downtime completed successfully] - A period of downtime for the Trixie (AI4D) cluster is being scheduled for Monday, April 19th from 9:00am-3:00pm.  Due to the nature of the maintenance all jobs in the queue will be terminated at the start of the maintenance window.</li> <li>[RESOLVED] - December 17 - We are currently experiencing issues with Trixie head node performance as detailed in #35 Investigation pending.</li> </ul>"},{"location":"Trixie-Status/#notes","title":"Notes","text":""},{"location":"jobs-abinit/","title":"abinit","text":"<p>This examples will show you how to setup and prepare an environment for Abinit jobs using conda on Trixie:</p>"},{"location":"jobs-abinit/#1-use-existing-miniconda-install-of-abinit","title":"1. Use existing miniconda install of abinit:","text":"<pre><code>#!/bin/bash\n# load spack\nmodule load spack\n# load the miniconda module\nmodule load miniconda3-4.8.2-gcc-9.2.0-sbqd2xu\n# create a conda environment named abinit \nconda create --name abinit python=3.7\nsource activate abinit\n# install pytorch dependencies via conda\nconda install abinit cudatoolkit=10.1 -c \n</code></pre>"},{"location":"jobs-abinit/#2-copy-create-a-test-abinit-file-si_208files","title":"2. Copy / create a test abinit file: Si_208.files","text":"<pre><code>cd $work_dir; cat &lt;&lt;_EOF_ &gt;Si_208.files\nSi_208.in\nSi_208.out\nSi_208\ntmp\nSi_208\n14-Si.LDA.fhi\n_EOF_\n</code></pre>"},{"location":"jobs-abinit/#3a-create-a-job-submission-script-test_abinitsh","title":"3(a). Create a job submission script: test_abinit.sh","text":"<pre><code>#!/bin/bash\n#SBATCH --nodes=8             # &lt;-- request # nodes from cluster partition\n##BATCH --ntasks-per-node=1   # &lt;-- request # tasks per node (default 1 cpu/task)\n#SBATCH --cpus-per-task=8     # &lt;-- request # cpus/task (OMP_NUM_THREADS)\n#SBATCH --mem=24000           # &lt;-- request 24000MB to run this job.\n#SBATCH --time=160            # &lt;-- request secs of wallclock/run.\n#SBATCH -p TrixieMain         # &lt;-- partition\n# \nexport HOSTFILE=\"/tmp/hosts.$SLURM_JOB_ID\"\nsrun hostname -s &gt; $HOSTFILE\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\n# --- Set the working directory using full path ---\nwork_dir=\"/gpfs/share/opt/apps/abinit/examples/Si_208\"\n# --- Set the input filename for the abinit program (full path)\nfiles=\"$work_dir/Si_208.files\"\n\necho \"Loading.. \"\necho spack load miniconda3\nspack load miniconda3@4.8.2\necho . activate abinit\n. activate /gpfs/share/opt/apps/abinit/conda/envs/abinit\necho which abinit\nwhich abinit\n\n#  Set the abinit program using full path:\nbin=`which abinit`\nif [ $? -ne 0 ]; then\n    echo \"abinit program not found!\"\n    exit 1\nfi\nif [ ! -x $bin ]; then\n    echo \"$bin not found!\"\n    exit 1\nfi\n\n# --- Sleep to make sure loaded properly on all nodes ---\nsleep 10\n\n# --- Start MPI jobs ---\nset -x\nwhich mpirun\n\nif [ -z \"$SLURM_NPROCS\" ]; then\n   if [ -z \"$SLURM_NTASKS_PER_NODE\" ]; then\n        SLURM_NTASKS_PER_NODE=1\n   fi\n   SLURM_NPROCS=$[ $SLURM_JOB_NUM_NODES * $SLURM_NTASKS_PER_NODE ]\nfi\n\nmpirun -f $HOSTFILE -n $SLURM_NPROCS -launcher ssh -wdir $work_dir $bin &lt; $files\n\ntest -r $HOSTFILE &amp;&amp; rm $HOSTFILE\n# -----------------------------------------------------------------------------\n</code></pre>"},{"location":"jobs-abinit/#3b-or-module-load-abinit-from-cc-stack","title":"3(b). OR Module load abinit from CC stack:","text":"<pre><code>$ module avail abinit\n\n-------------------------------- MPI-dependent avx512 modules ---------------------------------\n   abinit/8.4.4 (chem)    abinit/8.10.2 (chem)    abinit/9.2.1 (chem,D)\n\n  Where:\n   chem:  Chemistry libraries/apps / Logiciels de chimie\n   D:     Default Module\n\n$ module load abinit/8.10.2\n</code></pre> <pre><code>#\n#  abinit_template_slurm.job\n#\n#  Set scheduler parameters\n# -----------------------------------------------------------------------------\n#  --- Send email to address defined below when job is completed or aborted ---\n#SBATCH --mail-type=abort,end\n#\n#  --- Please replace with your email ---\n#SBATCH --mail-user=First.Lastname@cnrc-nrc.gc.ca\n#\n#  The next options define the running environment requested for this job.\n# -----------------------------------------------------------------------------\n#SBATCH --nodes=8             # &lt;-- request # nodes from cluster partition\n#SBATCH --ntasks-per-node=1   # &lt;-- request # tasks per node (default 1 cpu/task)\n#SBATCH --cpus-per-task=8     # &lt;-- request # cpus/task (OMP_NUM_THREADS)\n#SBATCH --mem=24000           # &lt;-- request 24000MB to run this job.\n#SBATCH --time=160            # &lt;-- request secs of wallclock/run.\n#\n#SBATCH -p TrixieMain         # &lt;-- partition\n#\n# -----------------------------------------------------------------------------\n#\n\n# --- Set the working directory using full path ---\nwork_dir=\"/gpfs/share/opt/apps/abinit/examples/Si_208\"\n# --- Set the input filename for the abinit program (full path)\nfiles=\"$work_dir/Si_208.files\"\n\nmodule load abinit/8.10.2\n\nbin=`which abinit`\n\n\nif [ -z \"$SLURM_NPROCS\" ]; then\n   if [ -z \"$SLURM_NTASKS_PER_NODE\" ]; then\n        SLURM_NTASKS_PER_NODE=1\n   fi\n   SLURM_NPROCS=$[ $SLURM_JOB_NUM_NODES * $SLURM_NTASKS_PER_NODE ]\nfi\n\nmpirun -n $SLURM_NPROCS -wdir $work_dir $bin &lt; $files\n\n</code></pre> <p>CC stack cannot yet module load abinit/9.2.1 (default module), as the StdEnv/2020 and libxc/5.0.0 are required for that module. There is no abinit/9.2.1 built using gcc/9.3.0, rather only intel compiler. Instead load the current supported version tested on trixie:</p> <p>Attempt to load abinit/9.2.1 w/ older env:</p> <pre><code>$ module spider abinit/9.2.1\n$ module load       nixpkgs/16.09  intel/2018.3  openmpi/3.1.4\n$ module load abinit/9.2.1 \n$ which abinit\n/cvmfs/soft.computecanada.ca/easybuild/software/2017/avx512/MPI/intel2018.3/openmpi3.1/abinit/9.2.1/bin/abinit\n\n</code></pre>"},{"location":"jobs-abinit/#4-submit-job-for-execution","title":"4. Submit job for execution:","text":"<pre><code>sbatch test_abinit.sh\n</code></pre> <p>Output will be 'Submitted batch job XXXXX'</p>"},{"location":"jobs-abinit/#5-confirm-execution-results","title":"5. Confirm execution results:","text":"<p>Local directory will contain a file 'slurm-XXXXX.out' which is the output of the job (stdout).</p> <p>Output should be:</p> <pre><code>cnNNN\ncnNNN\n..\nLoading.. \nspack load miniconda3@4.8.2\n. activate abinit\nwhich abinit\n/gpfs/share/opt/apps/abinit/conda/envs/abinit/bin/abinit\n+ which mpirun\n/gpfs/share/opt/apps/abinit/conda/envs/abinit/bin/mpirun\n+ mpirun -f /tmp/hosts.61574 -n 8 -launcher ssh -wdir /gpfs/share/opt/apps/abinit/examples/Si_208 /gpfs/share/opt/apps/abinit/conda/envs/abinit/bin/abinit\n  ABINIT 9.0.4\n</code></pre>"},{"location":"jobs-conda-RAPIDS/","title":"conda-RAPIDS","text":"<p>This examples will show you how to setup and prepare an environment for RAPID jobs using conda on Trixie:</p>"},{"location":"jobs-conda-RAPIDS/#1-create-a-rapids-miniconda-environment-copy-and-paste-the-next-3-sections-in-a-terminal-execute-individually","title":"1. Create a rapids miniconda environment. Copy and paste the next 3 sections in a terminal. Execute individually:","text":"<pre><code># ONLY USE IF YOU NEED TO REMOVE THE Python ENV (commented for safety) &amp;&amp; \\\n# ==================================================================== &amp;&amp; \\\n#conda deactivate &amp;&amp; \\\n#conda remove --name rapids --all\n</code></pre> <pre><code># Load Modules and setup Python ENV &amp;&amp; \\\n# ================================= &amp;&amp; \\\n# Some of these modules may not be necessary &amp;&amp; \\\n# You will have to do your own testing if you want a minimal &amp;&amp; \\\n# module selection &amp;&amp; \\\nmodule purge  &amp;&amp; \\\nmodule load miniconda3-4.8.2-gcc-9.2.0-sbqd2xu &amp;&amp; \\\nmodule load python-3.8.3-gcc-9.2.0-qxa3ikk &amp;&amp; \\\nmodule load cuda-10.1.168-gcc-9.2.0-xyykr4t &amp;&amp; \\\nmodule load gcc-9.2.0-gcc-9.2.0-vzr5o5q &amp;&amp; \\\nmodule load autoconf-2.69-gcc-9.2.0-qbibewz &amp;&amp; \\\nmodule load automake-1.16.2-gcc-9.2.0-6m76nfe &amp;&amp; \\\nconda create -n rapids python=3.8  &amp;&amp; \\\nconda activate rapids\n</code></pre> <pre><code># Install RAPIDS in rapids ENV &amp;&amp; \\\n# ============================ &amp;&amp; \\\nconda install -c rapidsai -c nvidia -c conda-forge \\\n    -c defaults rapids=0.15 python=3.8 cudatoolkit=10.1\n</code></pre>"},{"location":"jobs-conda-RAPIDS/#2-create-a-test-rapids-python-script-test_rapidspy","title":"2. Create a test RAPIDS python script: test_rapids.py","text":"<pre><code>import cudf\nimport numpy as np\n\ns = cudf.Series([1,2,3,None,4])\nprint(s)\n\ndf = cudf.DataFrame({'a': list(range(20)),\n                     'b': list(reversed(range(20))),\n                     'c': list(range(20))\n                    })\nprint(df.head(20))\n\n\nprint()\nprint()\n\n# Simple UDF\ndef simple_udf(x):\n    if x &gt; 0:\n        return x + 5\n    else:\n        return x - 5\n\n\n# Should run on GPU\nprint(df['a'].applymap(simple_udf))\n\nprint('... done ...')\n</code></pre>"},{"location":"jobs-conda-RAPIDS/#3-create-a-job-submission-script-test_rapidsslurm","title":"3. Create a job submission script: test_rapids.slurm","text":"<pre><code>#!/bin/bash\n# This script was modified for RAPIDS testing purposes... \n# Original script : https://github.com/ai4d-iasc/trixie/wiki/Jobs-conda-pytorch\n# Specify the partition of the cluster to run on\n#SBATCH --partition=JobTesting\n# Add your project account code using -A or --account\n#SBATCH --account ai4d-bio-04c\n# Specify the time allocated to the job. (30 mins just for kicks)\n#SBATCH --time=00:30:00\n# Request GPUs for the job. In this case 1 GPU\n#SBATCH --gres=gpu:1\n# Print out the hostname that the jobs is running on\nhs=`hostname`\necho -e \"host name : $hs\"\necho -e \"\\n\"\necho -e \"\\n\"\n\n# Run nvidia-smi to ensure that the job sees the GPUs\n/usr/bin/nvidia-smi\n\necho -e \"\\n\"\necho -e \"\\n\"\n\n# Load the miniconda module on the compute node\nmodule load miniconda3-4.8.2-gcc-9.2.0-sbqd2xu\nmodule load python-3.8.3-gcc-9.2.0-qxa3ikk\nmodule load cuda-10.2.89-gcc-9.2.0-fa5atrg\nmodule load gcc-9.2.0-gcc-9.2.0-vzr5o5q\nmodule load autoconf-2.69-gcc-9.2.0-qbibewz\nmodule load automake-1.16.2-gcc-9.2.0-6m76nfe \nsource activate rapids\n\n# Launch our test python file\npython test_rapids.py\n</code></pre> <p>Output will be 'Submitted batch job XXXXX'</p>"},{"location":"jobs-conda-RAPIDS/#5-confirm-execution-results","title":"5. Confirm execution results:","text":"<p>Local directory will contain a file 'slurm-XXXXX.out' which is the output of the job (stdout).</p> <p>Output should be:</p> <pre><code>host name : cn135\n\n\n\n\nThu Sep 24 17:00:50 2020       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n| N/A   31C    P0    42W / 300W |      0MiB / 32480MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\n\n\n\n0       1\n1       2\n2       3\n3    &lt;NA&gt;\n4       4\ndtype: int64\n\n     a   b   c\n0    0  19   0\n1    1  18   1\n2    2  17   2\n3    3  16   3\n4    4  15   4\n5    5  14   5\n6    6  13   6\n7    7  12   7\n8    8  11   8\n9    9  10   9\n10  10   9  10\n11  11   8  11\n12  12   7  12\n13  13   6  13\n14  14   5  14\n15  15   4  15\n16  16   3  16\n17  17   2  17\n18  18   1  18\n19  19   0  19\n\n\n0     -5\n1      6\n2      7\n3      8\n4      9\n5     10\n6     11\n7     12\n8     13\n9     14\n10    15\n11    16\n12    17\n13    18\n14    19\n15    20\n16    21\n17    22\n18    23\n19    24\nName: a, dtype: int64\n... done ...\n</code></pre>"},{"location":"jobs-conda-jupyterlab/","title":"Experimentation - Jupyter Notebook","text":"<p>At the end of this tutorial you will have: * created a \"env\" directory inside a project directory with:   * an environment running keras using CPU(s)   * an environment running keras using GPU(s) * defined 2 slurm jobs starting jupyter making use of respective environment</p>"},{"location":"jobs-conda-jupyterlab/#creating-conda-environments","title":"Creating conda environments:","text":"<pre><code>#load miniconda module\nmodule load miniconda3-4.7.12.1-gcc-9.2.0-j2idqxp\n\n#define a project variable\nexport MY_PROJECT_ROOT=$HOME/sample_project\n\n#go to project root\ncd $MY_PROJECT_ROOT\n\n#create a directory for all the environments\nmkdir env\n\n#create the CPU variant\nconda create -c conda-forge -p $MY_PROJECT_ROOT/env/Covid-Net-cpu python=3 jupyterlab imutils opencv\nmatplotlib keras scikit-learn pandas\n\n#create the GPU variant\nconda create -c conda-forge -p $MY_PROJECT_ROOT/env/Covid-Net-gpu python=3 jupyterlab imutils opencv\nmatplotlib keras scikit-learn pandas tensorflow-gpu\n\n#create a directory to hold jobfiles\nmkdir $MY_PROJECT_ROOT/jobs\n</code></pre>"},{"location":"jobs-conda-jupyterlab/#example-of-a-my_project_rootjobsjupyter-cpujob-file","title":"example of a $MY_PROJECT_ROOT/jobs/jupyter-cpu.job file","text":"<pre><code>#!/bin/bash -l\n#SBATCH --account=covid-01 \n#SBATCH --partition=TrixieMain\n#SBATCH --time=04:00:00 ####MAXIMUM 48:00:00 on Trixie\n#SBATCH --job-name=My_Awesome_Jupyter.cpu ####Try to be a bit descriptive or use the comment if you prefer shorter job names\n##SBATCH --comment=\"Comment on job\" ####Optional comment\n#SBATCH --mem=5G\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=2\n#SBATCH --output=%x-%j.out\n\n##### To help debugging\n#set -x\n\nexport MY_PROJECT_ROOT=$HOME/sample_project\n\nmodule load miniconda3-4.7.12.1-gcc-9.2.0-j2idqxp\n\nsource activate $MY_PROJECT_ROOT/env/Covid-Net-cpu\njupyter-lab --ip=*\n</code></pre>"},{"location":"jobs-conda-jupyterlab/#example-of-a-my_project_rootjobsjupyter-gpujob-file","title":"example of a $MY_PROJECT_ROOT/jobs/jupyter-gpu.job file","text":"<pre><code>#!/bin/bash -l\n#SBATCH --account=covid-01 \n#SBATCH --partition=TrixieMain\n#SBATCH --gres=gpu:1\n#SBATCH --time=04:00:00 ####MAXIMUM 48:00:00 on Trixie\n#SBATCH --job-name=My_Awesome_Jupyter.gpu ####Try to be a bit descriptive or use the comment if you prefer shorter job names\n##SBATCH --comment=\"Comment on job\" ####Optional comment\n#SBATCH --mem=5G\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=2\n#SBATCH --output=%x-%j.out\n\n##### To help debugging\n#set -x\n\nexport MY_PROJECT_ROOT=$HOME/sample_project\n\nmodule load miniconda3-4.7.12.1-gcc-9.2.0-j2idqxp\n\nsource activate $MY_PROJECT_ROOT/env/Covid-Net-gpu\n\njupyter-lab --ip=*\n</code></pre>"},{"location":"jobs-conda-jupyterlab/#scheduling-a-gpu-enabled-jupyter-notebook","title":"Scheduling a GPU enabled Jupyter notebook","text":"<pre><code>#starting a GPU enabled Jupyter notebook\nsbatch $MY_PROJECT_ROOT/jobs/jupyter-gpu.job\nSubmitted batch job 4502\n\n#you will get a jobid look for your log file names based on the job-name and the jobid\ntail -f My_Awesome_Jupyter.gpu-4502.out\n\n#look for the Jupyter notebook output you will get the node it is running on and the port number it is listening on. for example:\n#[...]\nTo access the notebook, open this file in a browser:\nfile:///gpfs/home/paulp/.local/share/jupyter/runtime/nbserver-24429-open.html\nOr copy and paste one of these URLs:\nhttp://cn122:8888/?token=388199bb6ef0cad54ef195f1286301548fc15ec2a39eee3c\nor http://127.0.0.1:8888/?token=388199bb6ef0cad54ef195f1286301548fc15ec2a39eee3c\n\n#At that moment a GPU enabled Jupyter notebook is running on compute node \"cn122\" on port 8888\n#Using an ssh tunnel 8888:cn122:8888 (using the following ssh command from the hn2 command line prompt)\nssh -Y -R 8888:cn122:8888 hn2\n#would allow you to access the remote jupyter notebook by connecting to\n#http://127.0.0.1:8888/?token=388199bb6ef0cad54ef195f1286301548fc15ec2a39eee3c\n</code></pre> <p>:warning::warning::warning:</p>"},{"location":"jobs-conda-jupyterlab/#releasing-resources-for-others-to-use","title":"Releasing Resources for others to use...","text":"<pre><code>#Don't forget to release resources when done by canceling your job \nscancel 4502\n</code></pre> <p>:warning::warning::warning:</p>"},{"location":"jobs-conda-jupyterlab/#sample-python-script-that-shows-devices-available-to-use-by-tensorflow-220","title":"sample python script that shows devices available to use by tensorflow 2.2.0","text":"<pre><code>import tensorflow as tf\n\n#display tensorflow version\nprint(tf.__version__)\n\nfrom tensorflow.python.client import device_lib\n\n#output tensorflow devices\nprint(device_lib.list_local_devices())\n</code></pre>"},{"location":"jobs-conda-jupyterlab/#useful-tip","title":"Useful tip","text":"<p>:bulb::bulb::bulb: * check on the queue using \"squeue\" or \"squeue -u $USER\" - a running job will have the \"R\" state a job waiting to run will show \"PD\" in the state column * you can as some information on the partitions using \"sinfo\"  * you can get some details on accounting using \"sacct\" * The Terminal within jupyter-lab runs within your job so it is safe to use to perform monitoring or other tasks that could be bothersome to others if ran on the head node. If you intend to use a lot of resource like this consider raising how many cores/cpus you request and maybe the RAM also... * a nice one liner to run a monitoring task for the GPU, this command will run indefinitely each second and output the listed parameters.</p> <pre><code>nvidia-smi --query-gpu=timestamp,name,pci.bus_id,driver_version,pstate,pcie.link.gen.max,pcie.link.gen.current,temperature.gpu,utilization.gpu,utilization.memory,memory.total,memory.free,memory.used --format=csv -l 1\n</code></pre> <p>:bulb::bulb::bulb:</p>"},{"location":"jobs-conda-jupyterlab/#making-a-ssh-tunnel-straight-to-the-node","title":"Making a SSH Tunnel Straight to the Node","text":"<p>This is somewhat of a rehash of the previous section but here's another way by creating a tunnel straight to the node.</p> <pre><code>#!/bin/bash\n# vim:nowrap:\n\n#SBATCH --job-name=Jupyter\n#SBATCH --comment=\"Jupyter\"\n##SBATCH --comment=\"registry.maze.science.gc.ca/ssc-hpcs/generic-job:ubuntu22.04\"\n\n# On Trixie\n#SBATCH --partition=JobTesting\n#SBATCH --account=dt-mtp\n\n#SBATCH --gres=gpu:1\n#SBATCH --time=6:00:00\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=6\n#SBATCH --mem=96G\n#SBATCH --open-mode=append\n#SBATCH --requeue\n#SBATCH --signal=B:USR1@30\n#SBATCH --output=%x-%j.out\n\n# Fix SLURM environment variables.\nSLURM_JOB_CPUS_PER_NODE=${SLURM_JOB_CPUS_PER_NODE%%(*)}   # '24(x2)' =&gt; 24\nSLURM_TASKS_PER_NODE=${SLURM_TASKS_PER_NODE%%(*)}   # '4(x2)' =&gt; '4'\n\n# NOTE: We set OMP_NUM_THREADS or else we get the following Warning:\n# WARNING:torch.distributed.run:\n# *****************************************\n# Setting OMP_NUM_THREADS environment variable for each process to be 1 in\n# default, to avoid your system being overloaded, please further tune the\n# variable for optimal performance in your application as needed.\n# *****************************************\nexport OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-$(nproc)}\n\nsource /gpfs/projects/DT/mtp/models/WMT2020/opt/miniconda3/bin/activate \"\"\nconda activate ScientificPythonStack\njupyter notebook --no-browser --port 8889  --ip=*\n</code></pre> <p>Then start the notebook</p> <pre><code>sbatch jupyter.slurm\n</code></pre> <p>You will need to know on what node your job is running. Use <code>squeue</code> to get the worker node name. or you could also look at the log <code>tail -f Jupyter*.out</code> to grab the url.</p>"},{"location":"jobs-conda-jupyterlab/#ssh-tunnel","title":"SSH Tunnel","text":"<p>To create a tunnel with <code>Putty</code> replacing <code>cnXXX</code> with your worker's hostname: * right click putty's title bar * change settings * connection &gt; SSH&gt; tunnels * source port 8888 * destination cnXXX:8888 * CLICK ADD * then apply</p>"},{"location":"jobs-conda-jupyterlab/#finally-stop-your-worker","title":"Finally Stop your Worker","text":"<p>It is IMPORTANT to <code>scancel &lt;JOBID&gt;</code> when you are not using your jupyter notebook.</p>"},{"location":"jobs-python-virtualenv/","title":"python venv","text":"<p>This examples will show you how to setup and prepare an environment for Python tensoflow jobs using virtualenv on Trixie:</p>"},{"location":"jobs-python-virtualenv/#1-create-a-python-virtual-environment","title":"1. Create a python virtual environment:","text":"<p>Either run from the command line or create tf-py37-environment.sh and run it:</p> <p>file: tf-py37-environment.sh:</p> <pre><code>#!/bin/bash\n\n## 1.(a) load the CC python3.7 module &amp; required libs:\nmodule load python/3.7\nmodule load cuda/10.0 cudnn\nmodule load hdf5\n\n## -or- 1.(b) choose to load python3.8 instead:\n#module load python/3.8\n#module load cuda/10.1.243 cudnn\n#module load hdf5\n\n\n## 2. Create a path for venv\nmkdir -p ~/work/venv\n# or use the project path to share virtualenv:\n#mkdir -p ~/project/venv\n\n## 2.2. Confirm python version &amp; paths (to make sure modules loaded ok:\nmodule list\nwhich python\nwhich pip\npython --version\n\n## 3.1. Create a new virtual environment named tf-py37:\nvirtualenv ~/work/venv/tf-py37\n\n## 3.2. activate env:\nsource ~/work/venv/tf-py37/bin/activate  \n\n## 3.3. run pip to install tf 1.x:\npip install tensorflow-gpu==1.15.0\n\n## See also table: https://stackoverflow.com/questions/50622525/which-tensorflow-and-cuda-version-combinations-are-compatible\n# Trixie presently supports cuda versions: 10.0.x and 10.1.x:\n#   - TF 1 works with cuda 10.0.130\n#   - TF 2 works with cuda 10.1.243\n</code></pre> <p>NOTE: with virtualenv it is still first necessary (upon login/beginning of scripts) to <code>module load</code> for the python version, if it differs from defaults already loaded (in output of: <code>module list</code>).</p>"},{"location":"jobs-python-virtualenv/#2-create-a-test-python-virtualenv-script","title":"2. Create a test python virtualenv script","text":"<p>test-tf-py37.py:</p> <pre><code>import tensorflow as tf\nprint('Tensorflow version:', tf.__version__())\nif tf.test.gpu_device_name():\n  print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\nelse:\n  print(\"Could not find GPU. Please install GPU version of TF.\")\n</code></pre>"},{"location":"jobs-python-virtualenv/#3-create-a-job-script","title":"3. Create a job script","text":"<p>test-tf-py37.sh:</p> <pre><code>#!/bin/bash\n\n# Specify the partition of the cluster to run on (Typically TrixieMain)\n#SBATCH --partition=TrixieMain\n# Add your project account code using -A or --account\n#SBATCH --account ai4d\n# Specify the time allocated to the job. Max 12 hours on TrixieMain queue.\n#SBATCH --time=12:00:00\n# Request GPUs for the job. In this case 1 GPU\n#SBATCH --gres=gpu:1\n# Print out the hostname that the jobs is running on\nhostname\n# Run nvidia-smi to ensure that the job sees the GPUs\n/usr/bin/nvidia-smi\n\nmodule load python/3.7\nmodule load cuda/10.0 cudnn\nmodule load hdf5\n\nsource ~/work/venv/tf-py37/bin/activate  \n\n# Launch our test tensorflow python file\npython test-tf-py37.py\n</code></pre>"},{"location":"jobs-python-virtualenv/#4-submit-job-for-execution","title":"4. Submit job for execution:","text":"<p>Remember to run the job script on a compute node using srun or sbatch commands in order to first allocate gpu resources.  </p> <pre><code>sbatch test-tf-py37.sh\n\nsrun --gres=gpu:1 -n 1 --pty /bin/bash --login   \n$ test-tf-py37.sh\n</code></pre> <p>Output will be 'Submitted batch job XXXXX'</p>"},{"location":"jobs-python-virtualenv/#5-confirm-execution-results","title":"5. Confirm execution results:","text":"<p>Local directory will contain a file 'slurm-XXXXX.out' which is the output of the job (stdout).</p> <p>Output should be:</p> <pre><code>cnXXX - &lt;nodename&gt;\n&lt;Date&gt;\n+--------\n| NVIDIA-SMI XXXX...\n....\n(4 listed V100 GPUs number 0 to 3)\n\nTensorflow version: 1.15.0\nDefault GPU Device:  ..\n</code></pre>"}]}